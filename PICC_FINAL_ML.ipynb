{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************README FIRST***************************************\n",
    "#This is a sample code file, which should be fully functional, for the PICC ML project at \n",
    "#Cincinnati Children's Hospital Medical Center.  All correspondence should go to\n",
    "#Manan Shah (mas476@gmail.com).   \n",
    "#\n",
    "#BACKGROUND\n",
    "#n critically ill infants, the position of a peripherally inserted central catheter (PICC) \n",
    "#must be confirmed frequently, as the tip may move from its original position and \n",
    "#run the risk of hyperosmolar vascular damage or extravasation into surrounding spaces. \n",
    "#\n",
    "#Automated detection of PICC tip position holds great promise for alerting bedside clinicians \n",
    "#to non-central PICCs.  \n",
    "#\n",
    "#Objectives  \n",
    "#This research seeks to use natural language processing (NLP) and supervised machine learning (ML) \n",
    "#techniques to predict PICC tip position based primarily on text analysis of radiograph reports \n",
    "#from infants with an upper extremity PICC. \n",
    "#\n",
    "#To use this file: make sure you have Python 3.7 and scikit learn, spacy, pandas all properly\n",
    "#installed.   Then you need a \"JSON\" file of radiology reports with appropriate section (eg PICC line)\n",
    "#(X AXIS) with Y Axis being the label.  You can either do it binary (e.g central vs non central)\n",
    "#or run it as a 12 classification model - you only need to change your labels.\n",
    "#It would be trivial to modify it to take other types of files, databases, etc.  We just chose\n",
    "#JSON as an easily available and manipulable format.\n",
    "#\n",
    "#You can set your classifier and your hyperparameters in the grid, and the rest of the work\n",
    "#should be automatic.  We have created a function for pre-processing where you can do additional\n",
    "#preprocessing, though we did not do much pre-processing for our work.  Depending on the nature of \n",
    "#your inputs, more pre-processing might be warranted (e.g you may want to remove stop words).  Note\n",
    "#that there are lots of publically available functions for this which may be useful.\n",
    "#\n",
    "#Finally, once your model is saved, you can simply load it and use the 'predict' function to \n",
    "#test it out.  Note that some pretrained models are also available at https://www.picclocation.com\n",
    "#***********************************************************************************************\n",
    "\n",
    "\n",
    "#IMPORT OUR LIBRARIES\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#MAKE SURE WE HAVE ALL THE SKLEARN ML algorithms imported\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import word2vec\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#ignore warnings in result - comment out when trying new stuff!!\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "parser = English()\n",
    "\n",
    "\n",
    "#This is a tokenizer that you can use if you wanted.  This is for demonstration purposes only\n",
    "#This is *NOT* used in our final ML model, though we can add it if we need to\n",
    "def spacy_tokenizer(sentence):\n",
    "    \n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    #mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#There are two ways to approach this problem.  Currently, we have a file called \n",
    "#'train' and a file called 'test'.  The train file contains the ~70% of our records\n",
    "# that are for training, while the 'test' file contains the 30% of our records that are for \n",
    "#'testing'. So we will train on the training data, figure out what works best, and then test\n",
    "#on the final testing data\n",
    "df = pd.read_json('train.json')\n",
    "#Column names of the x axis and labels\n",
    "X_AXIS = 'report'\n",
    "Y_AXIS = 'final_location'\n",
    "X_train = df[X_AXIS] # the features we want to analyze\n",
    "y_train = df[Y_AXIS] # the labels, or answers, we want to test against\n",
    "\n",
    "\n",
    "\n",
    "#If you want to just have one file and do an automated random split, you can also do it \n",
    "#this way\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, shuffle=True)\n",
    "\n",
    "#*********************************************************************************\n",
    "#            CHANGE THIS EVERY TIME YOU WANT TO TEST A NEW MODEL\n",
    "#**********************************************************************************\n",
    "\n",
    "#Classifier = SVC() with kernel = 'linear' would also work and likely give similar results\n",
    "classifier = LinearSVC()\n",
    "\n",
    "#Set the parameters we want to test for our model\n",
    "param_grid = {'classifier__C': [0.1,0.2,1,10,100],\n",
    "              'classifier__multi_class': ['crammer_singer', 'ovr'],\n",
    "              'classifier__loss': ['squared_hinge', 'hinge']\n",
    "             }\n",
    "        \n",
    "#If we wanted to build neural network, just put the neural network classifier here, e.g. MLPC\n",
    "#and put the parameter grid here, eg:\n",
    "#Activation: tanh, relu \n",
    "#Solver: sgd, adam \n",
    "#Alpha: 0.0001, 0.05, 0.01 \n",
    "#Learning Rate: constant, adaptive \n",
    "#Hidden Layer Sizes: (50,50,50), 100,50,50 , 50,100,50 etc etc\n",
    "\n",
    "#This is your n-gram range - you can customize this to be a bigram, tri-gram, etc\n",
    "#Depending on the model and your input data, various ranges may work better\n",
    "bow_vector = CountVectorizer(ngram_range=(1,5))\n",
    "#tfidf_vector = TfidfVectorizer\n",
    "#*********************************************************************************\n",
    "#            END SECTION TO CHANGE - THE REST OF CODE SHOULD BE RELATIVELY SIMILAR IN BETWEEN MODELS\n",
    "#**********************************************************************************\n",
    "             \n",
    "              \n",
    "        \n",
    "\n",
    "#Now this is a pipeline that we built - you can add more functions\n",
    "#For example, if you want to pre-process your data, such as if you wanted to remove\n",
    "#stop words, you can add a function here that will do all of these things.\n",
    "#It would be very easy to implement.\n",
    "#For our purposes, we decided to do very minimal pre-processing (e.g not remove stop words)\n",
    "pipe = Pipeline([    ('vectorizer', bow_vector),\n",
    "                     ('classifier', classifier)])\n",
    "\n",
    "\n",
    "#10 cross-fold validaton, set train score to false if you don't want to see training set\n",
    "grid = GridSearchCV(pipe, param_grid, refit = True, verbose = 3, n_jobs=-1, cv=10, return_train_score=True, scoring='accuracy') \n",
    "\n",
    "# fitting the model for grid search \n",
    "grid.fit(X_train, y_train) \n",
    "\n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "\n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(grid.best_estimator_) \n",
    "              \n",
    "df_r = pd.DataFrame(grid.cv_results_)\n",
    "df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK now we have the best parameters, so now you have this model (grid) and then you p\n",
    "df = pd.read_json('final_data_test.json')\n",
    "#Column names of the x axis and labels\n",
    "X_AXIS = 'report'\n",
    "Y_AXIS = 'final_location'\n",
    "X_test = df[X_AXIS] # the features we want to analyze\n",
    "y_test = df[Y_AXIS] # the labels, or answers, we want to test against\n",
    "\n",
    "grid_predictions = grid.predict(X_test) \n",
    "\n",
    "# print classification report \n",
    "print(metrics.classification_report(y_test, grid_predictions))\n",
    "print(grid)\n",
    "\n",
    "#Print all the answers the model got wrong\n",
    "for input, prediction, label in zip(X_test, grid_predictions, y_test):\n",
    "    if prediction != label:\n",
    "        print(input, 'has been classified as ', prediction, 'and should be ', label, '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "#Now you can save your model, or train the model on the WHOLE DATASET for production use\n",
    "dump(grid, filenameM (\"NAME.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
